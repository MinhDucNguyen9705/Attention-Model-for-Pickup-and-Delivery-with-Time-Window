{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76591c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import math\n",
    "from typing import NamedTuple\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_heads,\n",
    "            input_dim,\n",
    "            embed_dim,\n",
    "            val_dim=None,\n",
    "            key_dim=None\n",
    "    ):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        if val_dim is None:\n",
    "            val_dim = embed_dim // num_heads\n",
    "        if key_dim is None:\n",
    "            key_dim = val_dim\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.input_dim = input_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.val_dim = val_dim\n",
    "        self.key_dim = key_dim\n",
    "\n",
    "        self.norm_factor = 1 / math.sqrt(key_dim)  # See Attention is all you need\n",
    "\n",
    "        self.W_query = nn.Parameter(torch.Tensor(num_heads, input_dim, key_dim))\n",
    "        self.W_key = nn.Parameter(torch.Tensor(num_heads, input_dim, key_dim))\n",
    "        self.W_value = nn.Parameter(torch.Tensor(num_heads, input_dim, val_dim))\n",
    "\n",
    "        self.W_out = nn.Parameter(torch.Tensor(num_heads, val_dim, embed_dim))\n",
    "\n",
    "        self.init_parameters()\n",
    "    \n",
    "    def init_parameters(self):\n",
    "\n",
    "        for param in self.parameters():\n",
    "            stdv = 1. / math.sqrt(param.size(-1))\n",
    "            param.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = x.view(batch_size, -1, self.num_heads, x.size(-1) // self.num_heads)\n",
    "        return x.permute(2, 0, 1, 3)  # (num_heads, batch_size, seq_len, dim_per_head)\n",
    "\n",
    "    def forward(self, q, h=None, mask=None):\n",
    "\n",
    "        if h is None:\n",
    "            h = q\n",
    "        \n",
    "        batch_size, graph_size, input_dim = h.shape\n",
    "        n_query = q.shape[1]\n",
    "\n",
    "        hflat = h.contiguous().view(-1, input_dim)\n",
    "        qflat = q.contiguous().view(-1, input_dim)\n",
    "\n",
    "        Q = torch.matmul(qflat, self.W_query)\n",
    "        K = torch.matmul(hflat, self.W_key)\n",
    "        V = torch.matmul(hflat, self.W_value)\n",
    "\n",
    "        Q = self.split_heads(Q, batch_size)\n",
    "        K = self.split_heads(K, batch_size)\n",
    "        V = self.split_heads(V, batch_size)\n",
    "\n",
    "        compatibility = self.norm_factor * torch.matmul(Q, K.transpose(2, 3))\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.view(1, batch_size, n_query, graph_size).expand_as(compatibility)\n",
    "            compatibility[mask] = -np.inf\n",
    "\n",
    "        attn = torch.softmax(compatibility, dim=-1)\n",
    "\n",
    "        if mask is not None:\n",
    "            attnc = attn.clone()\n",
    "            attnc[mask] = 0\n",
    "            attn = attnc\n",
    "        \n",
    "        heads = torch.matmul(attn, V)\n",
    "\n",
    "        out = torch.mm(\n",
    "            heads.permute(1, 2, 0, 3).contiguous().view(-1, self.num_heads * self.val_dim),\n",
    "            self.W_out.view(-1, self.embed_dim)\n",
    "        ).view(batch_size, n_query, self.embed_dim)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b1b9150",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalization(nn.Module):\n",
    "    def __init__(self, embed_dim, normalization='batch'):\n",
    "        super(Normalization, self).__init__()\n",
    "\n",
    "        if normalization == 'batch':\n",
    "            self.norm = nn.BatchNorm1d(embed_dim, affine=True)\n",
    "        elif normalization == 'layer':\n",
    "            self.norm = nn.InstanceNorm1d(embed_dim, affine=True)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported normalization type: {}\".format(normalization))\n",
    "\n",
    "    def init_parameters(self):\n",
    "\n",
    "        for name, param in self.named_parameters():\n",
    "            stdv = 1. / math.sqrt(param.size(-1))\n",
    "            param.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if isinstance(self.norm, nn.BatchNorm1d):\n",
    "            return self.norm(x.view(-1, x.shape[-1])).view(*x.shape)\n",
    "        elif isinstance(self.norm, nn.InstanceNorm1d):\n",
    "            return self.norm(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported normalization type: {}\".format(type(self.norm)))\n",
    "        \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, ff_dim):\n",
    "        super(FeedForward, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ff_dim = ff_dim\n",
    "\n",
    "        self.linear1 = nn.Linear(embed_dim, ff_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(ff_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f65cd78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads,\n",
    "        embed_dim,\n",
    "        ff_dim=512,\n",
    "        normalization='batch'\n",
    "    ):\n",
    "        super(MultiHeadAttentionLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            input_dim=embed_dim,\n",
    "            embed_dim=embed_dim\n",
    "        )\n",
    "\n",
    "        self.ff = FeedForward(embed_dim=embed_dim, ff_dim=ff_dim)\n",
    "\n",
    "        self.norm1 = Normalization(embed_dim=embed_dim, normalization=normalization)\n",
    "        self.norm2 = Normalization(embed_dim=embed_dim, normalization=normalization)\n",
    "        \n",
    "        # self.act1 = nn.Tanh()\n",
    "        # self.act2 = nn.Tanh()\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "\n",
    "        mha_out = self.mha(x, h=x, mask=mask)\n",
    "        x = x + mha_out\n",
    "        x = self.norm1(x)\n",
    "        # x = self.act1(x)\n",
    "\n",
    "        ff_out = self.ff(x)\n",
    "        x = x + ff_out\n",
    "        x = self.norm2(x)\n",
    "        # x = self.act2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8adb6aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "mha_layer = MultiHeadAttentionLayer(\n",
    "    num_heads=8,\n",
    "    embed_dim=128,\n",
    "    ff_dim=512,\n",
    "    normalization='batch'\n",
    ")\n",
    "x = torch.randn(32, 10, 128)\n",
    "y = mha_layer(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7125549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_heads,\n",
    "                 embed_dim,\n",
    "                 num_layers,\n",
    "                 node_dim=None,\n",
    "                 normalization='batch',\n",
    "                 ff_dim=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.init_embed = nn.Linear(node_dim, embed_dim) if node_dim is not None else None\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            MultiHeadAttentionLayer(\n",
    "                num_heads=num_heads,\n",
    "                embed_dim=embed_dim,\n",
    "                ff_dim=ff_dim,\n",
    "                normalization=normalization\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "\n",
    "        h = self.init_embed(x) if self.init_embed is not None else x\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, mask=mask)\n",
    "        \n",
    "        return h, h.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53dcee0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 128]) torch.Size([32, 128])\n"
     ]
    }
   ],
   "source": [
    "encoder = GraphAttentionEncoder(\n",
    "    num_heads=8,\n",
    "    embed_dim=128,\n",
    "    num_layers=3,\n",
    "    node_dim=2,\n",
    "    normalization='batch',\n",
    "    ff_dim=512\n",
    ")\n",
    "sample = torch.randn(32, 10, 2)\n",
    "out_node, out_graph = encoder(sample)\n",
    "print(out_node.shape, out_graph.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f509c84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionModelFixed(NamedTuple):\n",
    "    \n",
    "    node_embeddings: torch.Tensor\n",
    "    context_node_projected: torch.Tensor\n",
    "    glimpse_key: torch.Tensor\n",
    "    glimpse_val: torch.Tensor\n",
    "    logit_key: torch.Tensor\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        assert torch.is_tensor(key) or isinstance(key, slice)\n",
    "        return AttentionModelFixed(\n",
    "            node_embeddings=self.node_embeddings[key],\n",
    "            context_node_projected=self.context_node_projected[key],\n",
    "            glimpse_key=self.glimpse_key[:, key],  # dim 0 are the heads\n",
    "            glimpse_val=self.glimpse_val[:, key],  # dim 0 are the heads\n",
    "            logit_key=self.logit_key[key]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5df5420",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionModel(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 embed_dim,\n",
    "                 hidden_dim,\n",
    "                 problem,\n",
    "                 n_encode_layers=2,\n",
    "                 tanh_clipping=10.,\n",
    "                 mask_inner=True,\n",
    "                 mask_logits=True,\n",
    "                 normalization='batch',\n",
    "                 n_heads=8,\n",
    "                 checkpoint_encoder=False,\n",
    "                 shrink_size=None):\n",
    "        super(AttentionModel, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_encode_layers = n_encode_layers\n",
    "        self.tanh_clipping = tanh_clipping\n",
    "        self.decode_type = None\n",
    "        self.temp = 1.0\n",
    "        self.mask_inner = mask_inner\n",
    "        self.mask_logits = mask_logits\n",
    "        self.n_heads = n_heads\n",
    "        self.checkpoint_encoder = checkpoint_encoder\n",
    "        self.shrink_size = shrink_size\n",
    "        self.problem = problem\n",
    "\n",
    "        step_context_dim = embed_dim + 1\n",
    "        node_dim = 3\n",
    "\n",
    "        self.init_embed_depot = nn.Linear(2, embed_dim)\n",
    "        self.init_embed = nn.Linear(node_dim, embed_dim)\n",
    "\n",
    "        self.embedder = GraphAttentionEncoder(\n",
    "            num_heads=n_heads,\n",
    "            embed_dim=embed_dim,\n",
    "            num_layers=n_encode_layers,\n",
    "            # node_dim=node_dim,\n",
    "            normalization=normalization\n",
    "        )\n",
    "\n",
    "        self.project_node_embeddings = nn.Linear(embed_dim, 3 * embed_dim, bias=False)\n",
    "        self.project_fixed_context = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.project_step_context = nn.Linear(step_context_dim, embed_dim, bias=False)\n",
    "\n",
    "        self.project_out = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "    \n",
    "    def set_decode_type(self, decode_type, temp=None):\n",
    "        self.decode_type = decode_type\n",
    "        if temp is not None:\n",
    "            self.temp = temp\n",
    "        \n",
    "    def forward(self, input, return_pi=False):\n",
    "\n",
    "        if self.checkpoint_encoder and self.training:\n",
    "            embeddings, _ = checkpoint(self.embedder, self._init_embed(input))\n",
    "        else:\n",
    "            embeddings, _ = self.embedder(self._init_embed(input))\n",
    "        \n",
    "        _log_p, pi = self._inner(input, embeddings)\n",
    "\n",
    "        cost, mask = self.problem.get_costs(input, pi)\n",
    "\n",
    "        ll = self._calc_log_likelihood(_log_p, pi, mask)\n",
    "        if return_pi:\n",
    "            return cost, ll, pi\n",
    "        else:\n",
    "            return cost, ll\n",
    "    \n",
    "    def _calc_log_likelihood(self, _log_p, a, mask):\n",
    "\n",
    "        log_p = _log_p.gather(2, a.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        if mask is not None:\n",
    "            log_p[mask] = 0\n",
    "        \n",
    "        return log_p.sum(1)\n",
    "\n",
    "    def _init_embed(self, input):\n",
    "\n",
    "        return torch.cat(\n",
    "            (\n",
    "                self.init_embed_depot(input[:, :1, 0:2]),\n",
    "                self.init_embed(torch.cat(\n",
    "                    (\n",
    "                        input['coords'],\n",
    "                        input['demand'][:, :, None],\n",
    "                    ),\n",
    "                    dim=-1\n",
    "                ))\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def _inner(self, input, embeddings):\n",
    "\n",
    "        outputs = []\n",
    "        sequences = []\n",
    "\n",
    "        state = self.problem.make_state(input)\n",
    "\n",
    "        fixed = self._precompute(embeddings)\n",
    "\n",
    "        batch_size = embeddings.size(0)\n",
    "\n",
    "        i = 0\n",
    "        while not (self.shrink_size is None and state.all_finished()):\n",
    "\n",
    "            if self.shrink_size is not None:\n",
    "                unfinished = torch.nonzero(state.get_finished() == 0)\n",
    "                if len(unfinished) == 0:\n",
    "                    break\n",
    "                unfinished = unfinished[:, 0]\n",
    "\n",
    "                if 16 <= len(unfinished) <= state.ids.size(0) - self.shrink_size:\n",
    "                    state = state[unfinished]\n",
    "                    fixed = fixed[unfinished]\n",
    "            \n",
    "            log_p, mask = self._get_log_p(fixed, state)\n",
    "\n",
    "            selected = self._select_node(log_p.exp()[:, 0, :], mask[:, 0, :])\n",
    "\n",
    "            state = state.update(selected)\n",
    "\n",
    "            if self.shrink_size is not None and state.ids.size(0) < batch_size:\n",
    "                log_p_, selected_ = log_p, selected\n",
    "                log_p = log_p_.new_zeros(batch_size, *log_p_.size()[1:])\n",
    "                selected = selected_.new_zeros(batch_size)\n",
    "\n",
    "                log_p[state.ids[:, 0]] = log_p_\n",
    "                selected[state.ids[:, 0]] = selected_\n",
    "            \n",
    "            outputs.append(log_p[:, 0, :])\n",
    "            sequences.append(selected)\n",
    "\n",
    "            i += 1\n",
    "        \n",
    "        return torch.stack(outputs, 1), torch.stack(sequences, 1)\n",
    "    \n",
    "    def _select_node(self, probs, mask):\n",
    "\n",
    "        if self.decode_type == \"greedy\":\n",
    "            _, selected = probs.max(1)\n",
    "\n",
    "        elif self.decode_type == \"sampling\":\n",
    "            selected = probs.multinomial(1).squeeze(1)\n",
    "\n",
    "            while mask.gather(1, selected.unsqueeze(-1)).data.any():\n",
    "                print('Sampled bad values, resampling!')\n",
    "                selected = probs.multinomial(1).squeeze(1)\n",
    "\n",
    "        return selected\n",
    "\n",
    "    def _precompute(self, embeddings, num_steps=1):\n",
    "\n",
    "        graph_embed = embeddings.mean(1)\n",
    "        fixed_context = self.project_fixed_context(graph_embed)[:, None, :]\n",
    "\n",
    "        glimpse_key_fixed, glimpse_val_fixed, logit_key_fixed = \\\n",
    "            self.project_node_embeddings(embeddings[:, None, :, :]).chunk(3, dim=-1)\n",
    "\n",
    "        fixed_attention_node_data = (\n",
    "            self._make_heads(glimpse_key_fixed, num_steps),\n",
    "            self._make_heads(glimpse_val_fixed, num_steps),\n",
    "            logit_key_fixed.contiguous()\n",
    "        )\n",
    "        return AttentionModelFixed(embeddings, fixed_context, *fixed_attention_node_data)\n",
    "    \n",
    "    def _get_parallel_step_context(self, embeddings, state, from_depot=False):\n",
    "\n",
    "        current_node = state.get_current_node()\n",
    "        batch_size, num_steps = current_node.size()\n",
    "\n",
    "        if from_depot:\n",
    "            return torch.cat(\n",
    "                (\n",
    "                    embeddings[:, 0:1, :].expand(batch_size, num_steps, embeddings.size(-1)),\n",
    "                    self.problem.VEHICLE_CAPACITY - torch.zeros_like(state.used_capacity[:, :, None])\n",
    "                ),\n",
    "                dim=-1\n",
    "            )\n",
    "\n",
    "    def _get_log_p_topk(self, fixed, state, k=None, normalize=True):\n",
    "        log_p, _ = self._get_log_p(fixed, state, normalize=normalize)\n",
    "\n",
    "        if k is not None and k < log_p.size(-1):\n",
    "            return log_p.topk(k, -1)\n",
    "\n",
    "        return (\n",
    "            log_p,\n",
    "            torch.arange(log_p.size(-1), device=log_p.device, dtype=torch.int64).repeat(log_p.size(0), 1)[:, None, :]\n",
    "        )\n",
    "\n",
    "    def _get_log_p(self, fixed, state, normalize=True):\n",
    "\n",
    "        query = fixed.context_node_projected + \\\n",
    "                self.project_step_context(self._get_parallel_step_context(fixed.node_embeddings, state))\n",
    "\n",
    "        glimpse_K, glimpse_V, logit_K = self._get_attention_node_data(fixed, state)\n",
    "\n",
    "        mask = state.get_mask()\n",
    "\n",
    "        log_p, glimpse = self._one_to_many_logits(query, glimpse_K, glimpse_V, logit_K, mask)\n",
    "\n",
    "        if normalize:\n",
    "            log_p = torch.log_softmax(log_p / self.temp, dim=-1)\n",
    "\n",
    "        assert not torch.isnan(log_p).any()\n",
    "\n",
    "        return log_p, mask\n",
    "    \n",
    "    def _one_to_many_logits(self, query, glimpse_K, glimpse_V, logit_K, mask):\n",
    "\n",
    "        batch_size, num_steps, embed_dim = query.size()\n",
    "        key_size = val_size = embed_dim // self.n_heads\n",
    "\n",
    "        # Compute the glimpse, rearrange dimensions so the dimensions are (n_heads, batch_size, num_steps, 1, key_size)\n",
    "        glimpse_Q = query.view(batch_size, num_steps, self.n_heads, 1, key_size).permute(2, 0, 1, 3, 4)\n",
    "\n",
    "        # Batch matrix multiplication to compute compatibilities (n_heads, batch_size, num_steps, graph_size)\n",
    "        compatibility = torch.matmul(glimpse_Q, glimpse_K.transpose(-2, -1)) / math.sqrt(glimpse_Q.size(-1))\n",
    "        if self.mask_inner:\n",
    "            assert self.mask_logits, \"Cannot mask inner without masking logits\"\n",
    "            compatibility[mask[None, :, :, None, :].expand_as(compatibility)] = -math.inf\n",
    "\n",
    "        # Batch matrix multiplication to compute heads (n_heads, batch_size, num_steps, val_size)\n",
    "        heads = torch.matmul(torch.softmax(compatibility, dim=-1), glimpse_V)\n",
    "\n",
    "        # Project to get glimpse/updated context node embedding (batch_size, num_steps, embedding_dim)\n",
    "        glimpse = self.project_out(\n",
    "            heads.permute(1, 2, 3, 0, 4).contiguous().view(-1, num_steps, 1, self.n_heads * val_size))\n",
    "\n",
    "        # Now projecting the glimpse is not needed since this can be absorbed into project_out\n",
    "        # final_Q = self.project_glimpse(glimpse)\n",
    "        final_Q = glimpse\n",
    "        # Batch matrix multiplication to compute logits (batch_size, num_steps, graph_size)\n",
    "        # logits = 'compatibility'\n",
    "        logits = torch.matmul(final_Q, logit_K.transpose(-2, -1)).squeeze(-2) / math.sqrt(final_Q.size(-1))\n",
    "\n",
    "        # From the logits compute the probabilities by clipping, masking and softmax\n",
    "        if self.tanh_clipping > 0:\n",
    "            logits = torch.tanh(logits) * self.tanh_clipping\n",
    "        if self.mask_logits:\n",
    "            logits[mask] = -math.inf\n",
    "\n",
    "        return logits, glimpse.squeeze(-2)\n",
    "\n",
    "    def _get_attention_node_data(self, fixed, state):\n",
    "\n",
    "        if self.is_vrp and self.allow_partial:\n",
    "\n",
    "            # Need to provide information of how much each node has already been served\n",
    "            # Clone demands as they are needed by the backprop whereas they are updated later\n",
    "            glimpse_key_step, glimpse_val_step, logit_key_step = \\\n",
    "                self.project_node_step(state.demands_with_depot[:, :, :, None].clone()).chunk(3, dim=-1)\n",
    "\n",
    "            # Projection of concatenation is equivalent to addition of projections but this is more efficient\n",
    "            return (\n",
    "                fixed.glimpse_key + self._make_heads(glimpse_key_step),\n",
    "                fixed.glimpse_val + self._make_heads(glimpse_val_step),\n",
    "                fixed.logit_key + logit_key_step,\n",
    "            )\n",
    "\n",
    "        # TSP or VRP without split delivery\n",
    "        return fixed.glimpse_key, fixed.glimpse_val, fixed.logit_key\n",
    "\n",
    "    def _make_heads(self, v, num_steps=None):\n",
    "        assert num_steps is None or v.size(1) == 1 or v.size(1) == num_steps\n",
    "\n",
    "        return (\n",
    "            v.contiguous().view(v.size(0), v.size(1), v.size(2), self.n_heads, -1)\n",
    "            .expand(v.size(0), v.size(1) if num_steps is None else num_steps, v.size(2), self.n_heads, -1)\n",
    "            .permute(3, 0, 1, 2, 4)  # (n_heads, batch_size, num_steps, graph_size, head_dim)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a57404f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
